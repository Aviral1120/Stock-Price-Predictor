# -*- coding: utf-8 -*-
"""Stock Price Predictor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ayZhaZFhld9Lm3HesD5KZmttXDIVCZ7
"""

pip install --upgrade scikit-learn

# importing all the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn import metrics 
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

 
import warnings
warnings.filterwarnings('ignore')

# reading the data and storing it in a dataframe
df = pd.read_csv('/content/TSLAA.csv')
df.head()

# checking the no of rows and columns in the data
df.shape

df.describe()

# checking the data types of the data
df.info()

# Exploratory Data Analysis

plt.figure(figsize=(15,5))
plt.plot(df['Close'])
plt.title('Tesla Close price.', fontsize=15)
plt.ylabel('Price in dollars.')
plt.show()

# checking a few entries
df.head()

# comparing the columns for the similar data
# data redundancy
# data cleaning
df[df['Close'] == df['Adj Close']].shape

# removing the unnecessary column
df = df.drop(['Adj Close'], axis=1)

# checking the irregularities in data
df.isnull().sum()

# distribution plot
features = ['Open', 'High', 'Low', 'Close', 'Volume']

plt.subplots(figsize=(20,10))

for i, col in enumerate(features):
    plt.subplot(2,3,i+1)
    sb.distplot(df[col])
plt.show()

# two peaks in OHLC
# data has varied significantly in two regions
# volume data is left skewed

# box plots
plt.subplots(figsize=(20,10))
for i, col in enumerate(features):
    plt.subplot(2,3,i+1)
    sb.boxplot(df[col])
plt.show()

# volume data has the maximum outliers

# Feature Engineering
# adding some extra columns by splitting the 'date' column
splitted = df['Date'].str.split('-', expand=True)

df['day'] = splitted[1].astype('int')
df['month'] = splitted[0].astype('int')
df['year'] = splitted[2].astype('int')

df.head()

# adding another column for quarter end
# a year consits of 4 quarters
df['is_quarter_end'] = np.where(df['month']%3==0,1,0)
df.head()

# bar graph
# data_grouped = df.groupby('year').mean()
plt.subplots(figsize=(20,10))
 
for i, col in enumerate(['Open', 'High', 'Low', 'Close']):
  plt.subplot(2,2,i+1)
  data_grouped[col].plot.bar()
plt.show()

# from 2013 to 2014 the stock prices are nearly doubled

df.groupby('is_quarter_end').mean()
# stock prices are quite high at the quarter ends

# added more columns
# open and close difference
# high and low difference
# target which acts as the signal
df['open-close']  = df['Open'] - df['Close']
df['low-high']  = df['Low'] - df['High']
df['target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)

df.head()

# pie chart
# checking if the target is balanced
plt.pie(df['target'].value_counts().values,
        labels=[0, 1], autopct='%1.1f%%')
plt.show()

# heatmap
plt.figure(figsize=(10, 10))
 
# As our concern is with the highly
# correlated features only so, we will visualize
# our heatmap as per that criteria only.
sb.heatmap(df.corr() > 0.9, annot=True, cbar=False)
plt.show()

# Normalizing data to avoid redundancy
# splitting the data in to training and testing data
features = df[['open-close', 'low-high', 'is_quarter_end']]
target = df['target']
 
scaler = StandardScaler()
features = scaler.fit_transform(features)
 
X_train, X_valid, Y_train, Y_valid = train_test_split(
    features, target, test_size=0.1, random_state=2022)
print(X_train.shape, X_valid.shape)

# checking which algo will prove the best
models = [LogisticRegression(), SVC(
  kernel='poly', probability=True), XGBClassifier()]
 
for i in range(3):
  models[i].fit(X_train, Y_train)
 
  print(f'{models[i]} : ')
  print('Training Accuracy : ', metrics.roc_auc_score(
    Y_train, models[i].predict_proba(X_train)[:,1]))
  print('Validation Accuracy : ', metrics.roc_auc_score(
    Y_valid, models[i].predict_proba(X_valid)[:,1]))
  print()

  # Extremen Gradient Boost (XGB) is the most accurate algo to follow

# plotting the confusion matrix
# to tell how well our predictor is working 
clf = SVC(random_state=2022)
clf.fit(X_train, Y_train)
SVC(random_state=2022)
predictions = clf.predict(X_valid)
cm = confusion_matrix(Y_valid, predictions, labels=clf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)
disp.plot()
plt.show()

# end result
ap = 22+104
an = 18+98
pp = 98+104
pn = 18+22

print("Actual positive result :",ap)
print("Actual negative result :",an)
print("Predicted positive result :",pp)
print("Predicted negative result :",pn)

total_pos = 104
total_neg = 18
accuracy = (total_pos + total_neg)/(ap+an)
error = 1 - accuracy
precision = total_pos/pp

print("Accuracy :",accuracy)
print("Precision :",precision)
print("Error :",error)